{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import h5py\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = '/scratch1/rpujari/gcr_workspace/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_names = ['jigsaw-dataset', 'hate-speech-dataset', 'hate-speech-and-offensive-language', 'ami-ibereval-dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jigsaw_data(data_path):\n",
    "    train_file = list(csv.reader(open(data_path + '/train.csv')))\n",
    "\n",
    "    train_data = []\n",
    "    labels = {}\n",
    "\n",
    "    for rnum, row in enumerate(train_file):\n",
    "        if rnum == 0:\n",
    "            keys = row\n",
    "            for key in keys[2:]:\n",
    "                labels[key] = set()\n",
    "        else:\n",
    "            eg = {}\n",
    "            for i, val in enumerate(row):\n",
    "                eg[keys[i]] = val\n",
    "                if keys[i] in labels:\n",
    "                    labels[keys[i]].add(val)\n",
    "            train_data.append(eg)\n",
    "\n",
    "\n",
    "    test_file = list(csv.reader(open(data_path + '/test.csv')))\n",
    "    test_labels = list(csv.reader(open(data_path + '/test_labels.csv')))\n",
    "\n",
    "    rnum = 0\n",
    "    test_data = []\n",
    "    tlabels = {}\n",
    "    for row1, row2 in zip(test_file, test_labels):\n",
    "        if rnum == 0:\n",
    "            keys = row1 + row2\n",
    "            for key in row2[1:]:\n",
    "                tlabels[key] = set()\n",
    "        else:\n",
    "            eg = {}\n",
    "            for i, val in enumerate(row1):\n",
    "                eg[keys[i]] = val\n",
    "            for i, val in enumerate(row2):\n",
    "                eg[keys[len(row1) + i]] = val\n",
    "                if keys[len(row1) + i] in tlabels:\n",
    "                    tlabels[keys[len(row1) + i]].add(val)\n",
    "            test_data.append(eg)\n",
    "        rnum += 1\n",
    "        \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hate_speech_data(data_path):\n",
    "    fdata = list(csv.reader(open(data_path + '/annotations_metadata.csv')))\n",
    "    data = []\n",
    "    for rnum, row in enumerate(fdata):\n",
    "        if rnum > 0:\n",
    "            eg = {}\n",
    "            eg['id'] = row[0]\n",
    "            eg['comment_text'] = open(data_path + '/all_files/' + row[0] + '.txt').read().strip()\n",
    "            if row[4] == 'noHate':\n",
    "                eg['hate'] = 0\n",
    "            else:\n",
    "                eg['hate'] = 1\n",
    "            eg['num_contexts'] = row[3]\n",
    "            data.append(eg)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hate_speech_offensive_data(data_path):\n",
    "    fdata = list(csv.reader(open(data_path + 'labeled_data.csv', 'r')))\n",
    "    data = []\n",
    "    for row in fdata[1:]:\n",
    "        eg = {}\n",
    "        eg['comment_text'] = row[-1]\n",
    "        for j, ritem in enumerate(row[1:-1]):\n",
    "            eg[fdata[0][j + 1]] = ritem\n",
    "        data.append(eg)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_misogyny_data(data_path):\n",
    "    fdata = list(csv.reader(open(data_path + '/en_AMI_TrainingSet_NEW.csv', 'r', errors='ignore')))\n",
    "    data = []\n",
    "    for row in fdata[1:]:\n",
    "        eg = {}\n",
    "        eg['comment_text'] = row[1]\n",
    "        for j, ritem in enumerate(row):\n",
    "            if j != 1:\n",
    "                eg[fdata[0][j]] = ritem\n",
    "        data.append(eg)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_data = load_jigsaw_data(dpath + dset_names[0] + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_data = load_hate_speech_data(dpath + dset_names[1] + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hso_data = load_hate_speech_offensive_data(dpath + dset_names[2] + '/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "misogyny_data = load_misogyny_data(dpath + dset_names[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571 153164 10944 24783 3251\n"
     ]
    }
   ],
   "source": [
    "print(len(js_data[0]), len(js_data[1]), len(hs_data), len(hso_data), len(misogyny_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351713\n"
     ]
    }
   ],
   "source": [
    "all_sents = []\n",
    "for eg in js_data[0]:\n",
    "    all_sents.append(eg['comment_text'])\n",
    "for eg in js_data[1]:\n",
    "    all_sents.append(eg['comment_text'])\n",
    "for eg in hs_data:\n",
    "    all_sents.append(eg['comment_text'])\n",
    "for eg in hso_data:\n",
    "    all_sents.append(eg['comment_text'])\n",
    "for eg in misogyny_data:\n",
    "    all_sents.append(eg['comment_text'])\n",
    "print(len(all_sents))\n",
    "cuda_device = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'albert-large-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eb41a6094e4214a44ad001053dd065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6177bd3fceb8457ca35da762890e27e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=685.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fe14f77b8046eaaf44ae5acab9e6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=71509304.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_class = AlbertTokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "with torch.cuda.device(cuda_device):\n",
    "    with torch.no_grad():\n",
    "        model = AlbertModel.from_pretrained(model_name,\\\n",
    "                                          output_hidden_states=False,\\\n",
    "                                          output_attentions=False)\n",
    "        model.eval()\n",
    "        model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_emb(all_sents):\n",
    "    if len(all_sents) > 0:\n",
    "        with torch.cuda.device(cuda_device):\n",
    "            all_toks = tokenizer.batch_encode_plus(all_sents, padding=True,\\\n",
    "                                                   add_special_tokens=True)\n",
    "            torch.cuda.empty_cache()\n",
    "            tok_tensor = torch.tensor([l[:512] for l in all_toks['input_ids']]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                all_doc_tensor = model(tok_tensor)['last_hidden_state']\n",
    "                all_doc_tensor.to('cpu')\n",
    "            all_attn_mask = torch.tensor(all_toks['attention_mask'])\n",
    "            ret_tensor = torch.FloatTensor(all_doc_tensor.size(0), all_doc_tensor.size(-1))\n",
    "            for i in range(all_doc_tensor.size(0)):\n",
    "                slen = torch.sum(all_attn_mask[i, :])\n",
    "                ret_tensor[i, :] = torch.mean(all_doc_tensor[i, :slen, :], dim=0)\n",
    "            del tok_tensor\n",
    "            del all_doc_tensor\n",
    "            del all_attn_mask\n",
    "            torch.cuda.empty_cache()\n",
    "            return ret_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_bert_embs(all_sents, save_path=None, bsz=50):\n",
    "    b = 0\n",
    "    e = bsz\n",
    "    ret_vecs = []\n",
    "    t1 = datetime.now()\n",
    "    while b < len(all_sents):\n",
    "        batch_sents = all_sents[b:e]\n",
    "        out_tensor = create_bert_emb(batch_sents)\n",
    "        ret_vecs.append(out_tensor)\n",
    "        b += bsz\n",
    "        e += bsz\n",
    "        if b % 500 == 0:\n",
    "            t2 = datetime.now()\n",
    "            print(b, 'done', t2 - t1)\n",
    "            if save_path:\n",
    "                with open(save_path, 'wb') as outfile:\n",
    "                    pickle.dump(ret_vecs, outfile)\n",
    "    ret_vec = torch.cat(ret_vecs, dim=0)\n",
    "    if save_path:\n",
    "        with open(save_path, 'wb') as outfile:\n",
    "            pickle.dump(ret_vec, outfile)\n",
    "    return ret_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 done 0:00:29.646505\n",
      "1000 done 0:00:58.043672\n",
      "1500 done 0:01:27.255643\n",
      "2000 done 0:01:53.151553\n",
      "2500 done 0:02:20.834842\n",
      "3000 done 0:02:49.906549\n",
      "3500 done 0:03:20.720928\n",
      "4000 done 0:03:50.571965\n",
      "4500 done 0:04:19.020044\n",
      "5000 done 0:04:46.499958\n",
      "5500 done 0:05:16.634123\n",
      "6000 done 0:05:46.029051\n",
      "6500 done 0:06:15.236323\n",
      "7000 done 0:06:42.679575\n",
      "7500 done 0:07:09.117063\n",
      "8000 done 0:07:35.724320\n",
      "8500 done 0:08:04.641101\n",
      "9000 done 0:08:36.027787\n",
      "9500 done 0:09:04.087689\n",
      "10000 done 0:09:30.402630\n",
      "10500 done 0:10:00.878675\n",
      "11000 done 0:10:30.067722\n",
      "11500 done 0:11:00.420458\n",
      "12000 done 0:11:30.207525\n",
      "12500 done 0:11:58.373238\n",
      "13000 done 0:12:24.724016\n",
      "13500 done 0:12:55.205466\n",
      "14000 done 0:13:23.914582\n",
      "14500 done 0:13:53.917916\n",
      "15000 done 0:14:21.387391\n",
      "15500 done 0:14:48.566054\n",
      "16000 done 0:15:15.722616\n",
      "16500 done 0:15:42.691489\n",
      "17000 done 0:16:13.923926\n",
      "17500 done 0:16:44.022109\n",
      "18000 done 0:17:12.897033\n",
      "18500 done 0:17:41.882949\n",
      "19000 done 0:18:13.203501\n",
      "19500 done 0:18:40.677639\n",
      "20000 done 0:19:04.278328\n",
      "20500 done 0:19:34.696372\n",
      "21000 done 0:20:01.945399\n",
      "21500 done 0:20:31.019870\n",
      "22000 done 0:21:00.132760\n",
      "22500 done 0:21:28.990733\n",
      "23000 done 0:21:58.629927\n",
      "23500 done 0:22:28.093654\n",
      "24000 done 0:22:57.504893\n",
      "24500 done 0:23:22.594900\n",
      "25000 done 0:23:53.971733\n",
      "25500 done 0:24:24.326134\n",
      "26000 done 0:24:52.589571\n",
      "26500 done 0:25:20.108088\n",
      "27000 done 0:25:48.218250\n",
      "27500 done 0:26:17.413140\n",
      "28000 done 0:26:49.630081\n",
      "28500 done 0:27:19.734952\n",
      "29000 done 0:27:48.739743\n",
      "29500 done 0:28:19.290605\n",
      "30000 done 0:28:46.938783\n",
      "30500 done 0:29:16.118530\n",
      "31000 done 0:29:44.755194\n",
      "31500 done 0:30:15.287195\n",
      "32000 done 0:30:45.084503\n",
      "32500 done 0:31:17.383038\n",
      "33000 done 0:31:47.251156\n",
      "33500 done 0:32:17.451574\n",
      "34000 done 0:32:46.967456\n",
      "34500 done 0:33:15.482606\n",
      "35000 done 0:33:47.905302\n",
      "35500 done 0:34:19.149770\n",
      "36000 done 0:34:50.186209\n",
      "36500 done 0:35:20.933257\n",
      "37000 done 0:35:53.186971\n",
      "37500 done 0:36:24.610788\n",
      "38000 done 0:36:56.080208\n",
      "38500 done 0:37:28.642671\n",
      "39000 done 0:37:57.147049\n",
      "39500 done 0:38:24.903353\n",
      "40000 done 0:38:56.478256\n",
      "40500 done 0:39:25.769320\n",
      "41000 done 0:39:56.714550\n",
      "41500 done 0:40:28.294661\n",
      "42000 done 0:40:58.502055\n",
      "42500 done 0:41:27.454225\n",
      "43000 done 0:41:57.282772\n",
      "43500 done 0:42:26.954756\n",
      "44000 done 0:42:57.534149\n",
      "44500 done 0:43:27.614356\n",
      "45000 done 0:43:55.311977\n",
      "45500 done 0:44:24.949725\n",
      "46000 done 0:44:52.311929\n",
      "46500 done 0:45:24.472250\n",
      "47000 done 0:45:56.360061\n",
      "47500 done 0:46:27.849226\n",
      "48000 done 0:47:00.743639\n",
      "48500 done 0:47:33.412996\n",
      "49000 done 0:48:06.305079\n",
      "49500 done 0:48:36.301960\n",
      "50000 done 0:49:05.559013\n",
      "50500 done 0:49:38.650147\n",
      "51000 done 0:50:08.333461\n",
      "51500 done 0:50:38.928673\n",
      "52000 done 0:51:09.937000\n",
      "52500 done 0:51:41.847296\n",
      "53000 done 0:52:13.836350\n",
      "53500 done 0:52:46.313668\n",
      "54000 done 0:53:17.838508\n",
      "54500 done 0:53:49.920992\n",
      "55000 done 0:54:19.161632\n",
      "55500 done 0:54:51.776060\n",
      "56000 done 0:55:22.360548\n",
      "56500 done 0:55:53.988569\n",
      "57000 done 0:56:27.089323\n",
      "57500 done 0:56:57.398889\n",
      "58000 done 0:57:30.736049\n",
      "58500 done 0:58:02.575724\n",
      "59000 done 0:58:34.847366\n",
      "59500 done 0:59:05.365004\n",
      "60000 done 0:59:36.292887\n",
      "60500 done 1:00:08.635613\n",
      "61000 done 1:00:39.431636\n",
      "61500 done 1:01:13.438255\n",
      "62000 done 1:01:44.182951\n",
      "62500 done 1:02:15.851348\n",
      "63000 done 1:02:49.246084\n",
      "63500 done 1:03:21.564529\n",
      "64000 done 1:03:51.492370\n",
      "64500 done 1:04:25.041575\n",
      "65000 done 1:04:57.427292\n",
      "65500 done 1:05:26.785937\n",
      "66000 done 1:05:59.574584\n",
      "66500 done 1:06:30.272543\n",
      "67000 done 1:07:01.313036\n",
      "67500 done 1:07:33.058392\n",
      "68000 done 1:08:04.459195\n",
      "68500 done 1:08:35.253825\n",
      "69000 done 1:09:07.368385\n",
      "69500 done 1:09:38.258521\n",
      "70000 done 1:10:12.168242\n",
      "70500 done 1:10:42.700821\n",
      "71000 done 1:11:16.763836\n",
      "71500 done 1:11:47.253274\n",
      "72000 done 1:12:23.306964\n",
      "72500 done 1:12:55.127574\n",
      "73000 done 1:13:27.210042\n",
      "73500 done 1:13:57.432916\n",
      "74000 done 1:14:29.253097\n",
      "74500 done 1:15:00.169374\n",
      "75000 done 1:15:34.961005\n",
      "75500 done 1:16:08.796505\n",
      "76000 done 1:16:41.192469\n",
      "76500 done 1:17:14.146548\n",
      "77000 done 1:17:48.438392\n",
      "77500 done 1:18:21.295991\n",
      "78000 done 1:18:54.996687\n",
      "78500 done 1:19:27.192897\n",
      "79000 done 1:19:58.901387\n",
      "79500 done 1:20:31.218845\n",
      "80000 done 1:20:59.616793\n",
      "80500 done 1:21:31.806691\n",
      "81000 done 1:22:01.847241\n",
      "81500 done 1:22:36.278117\n",
      "82000 done 1:23:07.797393\n",
      "82500 done 1:23:40.772603\n",
      "83000 done 1:24:12.528486\n",
      "83500 done 1:24:42.079645\n",
      "84000 done 1:25:16.200159\n",
      "84500 done 1:25:51.380850\n",
      "85000 done 1:26:20.338999\n",
      "85500 done 1:26:51.317953\n",
      "86000 done 1:27:25.865539\n",
      "86500 done 1:27:57.222175\n",
      "87000 done 1:28:30.490126\n",
      "87500 done 1:29:08.507660\n",
      "88000 done 1:29:42.123816\n",
      "88500 done 1:30:16.470773\n",
      "89000 done 1:30:47.197254\n",
      "89500 done 1:31:17.706614\n",
      "90000 done 1:31:48.980948\n",
      "90500 done 1:32:20.038953\n",
      "91000 done 1:32:54.802964\n",
      "91500 done 1:33:28.023128\n",
      "92000 done 1:34:02.897563\n",
      "92500 done 1:34:35.757356\n",
      "93000 done 1:35:08.523398\n",
      "93500 done 1:35:38.106390\n",
      "95500 done 1:37:53.829036\n",
      "96000 done 1:38:25.506796\n",
      "96500 done 1:38:57.953932\n",
      "97000 done 1:39:29.414961\n",
      "97500 done 1:40:01.597901\n",
      "98000 done 1:40:33.793627\n",
      "98500 done 1:41:06.308361\n",
      "99000 done 1:41:37.395191\n",
      "99500 done 1:42:13.161374\n",
      "100000 done 1:42:49.118953\n",
      "100500 done 1:43:23.185210\n",
      "101000 done 1:43:55.863933\n",
      "101500 done 1:44:26.451099\n",
      "102000 done 1:45:01.010114\n",
      "102500 done 1:45:32.554067\n",
      "103000 done 1:46:06.863983\n",
      "103500 done 1:46:43.757051\n",
      "104000 done 1:47:19.741608\n",
      "104500 done 1:47:51.073920\n",
      "105000 done 1:48:25.523191\n",
      "105500 done 1:48:57.927459\n",
      "106000 done 1:49:31.566048\n",
      "106500 done 1:50:08.211644\n",
      "107000 done 1:50:46.142568\n",
      "107500 done 1:51:18.569872\n",
      "108000 done 1:51:52.672681\n",
      "108500 done 1:52:24.167413\n",
      "109000 done 1:52:56.315432\n",
      "109500 done 1:53:29.064686\n",
      "110000 done 1:54:04.369133\n",
      "110500 done 1:54:37.820506\n",
      "111000 done 1:55:16.527051\n",
      "111500 done 1:55:49.384202\n",
      "112000 done 1:56:23.104063\n",
      "112500 done 1:56:55.988214\n",
      "113000 done 1:57:27.293164\n",
      "113500 done 1:57:58.007703\n",
      "114000 done 1:58:31.215708\n",
      "114500 done 1:59:04.583085\n",
      "115000 done 1:59:44.767569\n",
      "115500 done 2:00:16.051403\n",
      "116000 done 2:00:50.247606\n",
      "116500 done 2:01:24.145489\n",
      "117000 done 2:01:57.846341\n",
      "117500 done 2:02:31.643169\n",
      "118000 done 2:03:07.129120\n",
      "118500 done 2:03:47.537743\n",
      "119000 done 2:04:27.221898\n",
      "119500 done 2:05:03.499108\n",
      "120000 done 2:05:34.075480\n",
      "120500 done 2:06:12.437592\n",
      "121000 done 2:06:49.542616\n",
      "121500 done 2:07:27.427295\n",
      "122000 done 2:08:07.352181\n",
      "122500 done 2:08:38.729582\n",
      "123000 done 2:09:18.151991\n",
      "123500 done 2:09:54.143200\n",
      "124000 done 2:10:29.832708\n",
      "124500 done 2:11:04.452493\n",
      "125000 done 2:11:38.691336\n",
      "125500 done 2:12:14.754832\n",
      "126000 done 2:12:47.526671\n",
      "126500 done 2:13:27.797274\n",
      "127000 done 2:14:05.257991\n",
      "127500 done 2:14:36.996694\n",
      "128000 done 2:15:11.506944\n",
      "128500 done 2:15:43.399547\n",
      "129000 done 2:16:17.416188\n",
      "129500 done 2:16:50.476347\n",
      "130000 done 2:17:23.068781\n",
      "130500 done 2:17:59.071783\n",
      "131000 done 2:18:34.943322\n",
      "131500 done 2:19:08.537234\n",
      "132000 done 2:19:45.745205\n",
      "132500 done 2:20:17.738091\n",
      "133000 done 2:20:54.433841\n",
      "133500 done 2:21:31.103558\n",
      "134000 done 2:22:03.522403\n",
      "134500 done 2:22:42.559601\n",
      "135000 done 2:23:22.220370\n",
      "135500 done 2:24:01.639703\n",
      "136000 done 2:24:39.964052\n",
      "136500 done 2:25:26.563433\n",
      "137000 done 2:26:05.259903\n",
      "137500 done 2:26:45.327023\n",
      "138000 done 2:27:25.801287\n",
      "138500 done 2:28:04.071067\n",
      "139000 done 2:28:44.740474\n",
      "139500 done 2:29:25.190292\n",
      "140000 done 2:30:12.709531\n",
      "140500 done 2:30:53.414175\n",
      "141000 done 2:31:34.990777\n",
      "141500 done 2:32:13.990646\n",
      "142000 done 2:32:53.908800\n",
      "142500 done 2:33:33.484421\n",
      "143000 done 2:34:09.003958\n",
      "143500 done 2:34:46.724604\n",
      "144000 done 2:35:24.045649\n",
      "144500 done 2:35:58.423008\n",
      "145000 done 2:36:34.878550\n",
      "145500 done 2:37:08.207306\n",
      "146000 done 2:37:43.736276\n",
      "146500 done 2:38:19.778439\n",
      "147000 done 2:38:51.681736\n",
      "147500 done 2:39:24.110511\n",
      "148000 done 2:40:01.023062\n",
      "148500 done 2:40:38.467626\n",
      "149000 done 2:41:14.473409\n",
      "149500 done 2:41:52.357264\n",
      "150000 done 2:42:25.622857\n",
      "150500 done 2:43:01.550211\n",
      "151000 done 2:43:35.454167\n",
      "151500 done 2:44:06.701164\n",
      "152000 done 2:44:40.944438\n",
      "152500 done 2:45:15.945445\n",
      "153000 done 2:45:50.126347\n",
      "153500 done 2:46:21.849622\n",
      "154000 done 2:46:55.262663\n",
      "154500 done 2:47:26.792275\n",
      "155000 done 2:47:57.660853\n",
      "155500 done 2:48:28.696040\n",
      "156000 done 2:49:02.031722\n",
      "156500 done 2:49:35.388780\n",
      "157000 done 2:50:08.322749\n",
      "157500 done 2:50:39.002935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158000 done 2:51:12.006482\n",
      "158500 done 2:51:45.824028\n",
      "159000 done 2:52:16.945337\n",
      "159500 done 2:52:50.671718\n",
      "160000 done 2:53:24.791908\n",
      "160500 done 2:53:57.950720\n",
      "161000 done 2:54:31.358849\n",
      "161500 done 2:55:05.564692\n",
      "162000 done 2:55:39.588681\n",
      "162500 done 2:56:11.491600\n",
      "163000 done 2:56:44.014347\n",
      "163500 done 2:57:17.730376\n",
      "164000 done 2:57:50.439243\n",
      "164500 done 2:58:25.262388\n",
      "165000 done 2:58:57.775578\n",
      "165500 done 2:59:31.180490\n",
      "166000 done 3:00:03.603777\n",
      "166500 done 3:00:38.449608\n",
      "167000 done 3:01:13.228034\n",
      "167500 done 3:01:44.142932\n",
      "168000 done 3:02:17.621151\n",
      "168500 done 3:02:51.823812\n",
      "169000 done 3:03:21.283509\n",
      "169500 done 3:03:55.437545\n",
      "170000 done 3:04:25.560633\n",
      "170500 done 3:04:57.791950\n",
      "171000 done 3:05:30.863844\n",
      "171500 done 3:06:00.540850\n",
      "172000 done 3:06:34.814107\n",
      "172500 done 3:07:08.697999\n",
      "173000 done 3:07:45.469327\n",
      "173500 done 3:08:23.886130\n",
      "174000 done 3:08:56.615415\n",
      "174500 done 3:09:28.599525\n",
      "175000 done 3:09:58.772095\n",
      "175500 done 3:10:38.419765\n",
      "176000 done 3:11:12.202746\n",
      "176500 done 3:11:47.921345\n",
      "177000 done 3:12:22.923733\n",
      "177500 done 3:12:55.268758\n",
      "178000 done 3:13:28.211242\n",
      "178500 done 3:14:00.546863\n",
      "179000 done 3:14:32.491275\n",
      "179500 done 3:15:06.084924\n",
      "180000 done 3:15:40.170529\n",
      "180500 done 3:16:11.410231\n",
      "181000 done 3:16:42.785202\n",
      "181500 done 3:17:10.244421\n",
      "182000 done 3:17:46.023226\n",
      "182500 done 3:18:18.816962\n",
      "183000 done 3:18:50.759905\n",
      "183500 done 3:19:23.932478\n",
      "184000 done 3:19:57.111688\n",
      "184500 done 3:20:27.747476\n",
      "185000 done 3:21:02.631752\n",
      "185500 done 3:21:38.106452\n",
      "186000 done 3:22:12.536477\n",
      "186500 done 3:22:45.875244\n",
      "187000 done 3:23:20.879657\n",
      "187500 done 3:23:55.387747\n",
      "188000 done 3:24:30.567738\n",
      "188500 done 3:25:03.325155\n",
      "189000 done 3:25:36.997812\n",
      "189500 done 3:26:10.338572\n",
      "190000 done 3:26:44.933097\n",
      "190500 done 3:27:19.530232\n",
      "191000 done 3:27:57.394487\n",
      "191500 done 3:28:31.103113\n",
      "192000 done 3:29:02.938901\n",
      "192500 done 3:29:35.687470\n",
      "193000 done 3:30:09.000775\n",
      "193500 done 3:30:43.445860\n",
      "194000 done 3:31:19.564071\n",
      "194500 done 3:31:53.149636\n",
      "195000 done 3:32:27.326192\n",
      "195500 done 3:33:03.384522\n",
      "196000 done 3:33:36.471700\n",
      "196500 done 3:34:07.702481\n",
      "197000 done 3:34:43.948043\n",
      "197500 done 3:35:19.937610\n",
      "198000 done 3:35:58.935522\n",
      "198500 done 3:36:39.135628\n",
      "199000 done 3:37:21.086439\n",
      "199500 done 3:38:00.149815\n",
      "200000 done 3:38:41.713653\n",
      "200500 done 3:39:19.537980\n",
      "201000 done 3:39:57.709953\n",
      "201500 done 3:40:30.023082\n",
      "202000 done 3:41:05.334716\n",
      "202500 done 3:41:40.356623\n",
      "203000 done 3:42:17.386182\n",
      "203500 done 3:42:57.535063\n",
      "204000 done 3:43:37.745384\n",
      "204500 done 3:44:22.846285\n",
      "205000 done 3:45:07.418353\n",
      "205500 done 3:45:48.353917\n",
      "206000 done 3:46:31.414983\n",
      "206500 done 3:47:08.633980\n",
      "207000 done 3:47:43.696167\n",
      "207500 done 3:48:20.783429\n",
      "208000 done 3:48:58.271808\n",
      "208500 done 3:49:36.268070\n",
      "209000 done 3:50:18.146968\n",
      "209500 done 3:50:59.824136\n",
      "210000 done 3:51:44.179706\n",
      "210500 done 3:52:24.442929\n",
      "211000 done 3:53:07.729452\n",
      "211500 done 3:53:43.610132\n",
      "212000 done 3:54:18.617632\n",
      "212500 done 3:54:57.062227\n",
      "213000 done 3:55:35.288704\n",
      "213500 done 3:56:09.671843\n",
      "214000 done 3:56:48.657194\n",
      "214500 done 3:57:26.903801\n",
      "215000 done 3:58:03.221316\n",
      "215500 done 3:58:39.496274\n",
      "216000 done 3:59:17.266945\n",
      "216500 done 3:59:52.957267\n",
      "217000 done 4:00:28.133779\n",
      "217500 done 4:01:04.327413\n",
      "218000 done 4:01:41.495032\n",
      "218500 done 4:02:17.815471\n",
      "219000 done 4:02:56.105216\n",
      "219500 done 4:03:32.274210\n",
      "220000 done 4:04:08.093136\n",
      "220500 done 4:04:45.402751\n",
      "221000 done 4:05:21.942531\n",
      "221500 done 4:05:54.494743\n",
      "222000 done 4:06:31.546325\n",
      "222500 done 4:07:09.006970\n",
      "223000 done 4:07:44.644750\n",
      "223500 done 4:08:21.425776\n",
      "224000 done 4:08:56.813427\n",
      "224500 done 4:09:35.233613\n",
      "225000 done 4:10:13.782349\n",
      "225500 done 4:10:51.970765\n",
      "226000 done 4:11:31.506901\n",
      "226500 done 4:12:09.905210\n",
      "227000 done 4:12:45.101631\n",
      "227500 done 4:13:22.174255\n",
      "228000 done 4:13:58.555289\n",
      "228500 done 4:14:34.132642\n",
      "229000 done 4:15:11.622227\n",
      "229500 done 4:15:47.815742\n",
      "230000 done 4:16:23.549065\n",
      "230500 done 4:17:00.288848\n",
      "231000 done 4:17:39.446581\n",
      "231500 done 4:18:16.216865\n",
      "232000 done 4:18:51.858385\n",
      "232500 done 4:19:30.367456\n",
      "233000 done 4:20:07.310003\n",
      "233500 done 4:20:43.722890\n",
      "234000 done 4:21:22.697458\n",
      "234500 done 4:22:02.966763\n",
      "235000 done 4:22:44.763407\n",
      "235500 done 4:23:22.816878\n",
      "236000 done 4:24:03.269313\n",
      "236500 done 4:24:40.934847\n",
      "237000 done 4:25:18.583464\n",
      "237500 done 4:25:55.721561\n",
      "238000 done 4:26:33.570509\n",
      "238500 done 4:27:11.741296\n",
      "239000 done 4:27:50.285095\n",
      "239500 done 4:28:28.742207\n",
      "240000 done 4:29:05.784167\n",
      "240500 done 4:29:45.520493\n",
      "241000 done 4:30:25.183422\n",
      "241500 done 4:31:01.660505\n",
      "242000 done 4:31:42.596226\n",
      "242500 done 4:32:20.687401\n",
      "243000 done 4:32:58.334908\n",
      "243500 done 4:33:39.588584\n",
      "244000 done 4:34:22.797173\n",
      "244500 done 4:35:03.341406\n",
      "245000 done 4:35:42.842834\n",
      "245500 done 4:36:22.519577\n",
      "246000 done 4:37:01.484945\n",
      "246500 done 4:37:37.909284\n",
      "247000 done 4:38:17.300029\n",
      "247500 done 4:38:53.527365\n",
      "248000 done 4:39:31.941717\n",
      "248500 done 4:40:13.036539\n",
      "249000 done 4:40:50.855957\n",
      "249500 done 4:41:27.647249\n",
      "250000 done 4:42:02.521374\n",
      "250500 done 4:42:38.724922\n",
      "251000 done 4:43:16.232663\n",
      "251500 done 4:43:55.189166\n",
      "252000 done 4:44:33.160858\n",
      "252500 done 4:45:10.165631\n",
      "253000 done 4:45:51.272525\n",
      "253500 done 4:46:29.825047\n",
      "254000 done 4:47:09.642280\n",
      "254500 done 4:47:48.696926\n",
      "255000 done 4:48:26.038747\n",
      "255500 done 4:49:04.498799\n",
      "256000 done 4:49:43.376807\n",
      "256500 done 4:50:24.511608\n",
      "257000 done 4:51:02.676815\n",
      "257500 done 4:51:43.274716\n",
      "258000 done 4:52:21.346007\n",
      "258500 done 4:53:03.267650\n",
      "259000 done 4:53:42.127675\n",
      "259500 done 4:54:24.382037\n",
      "260000 done 4:55:03.873829\n",
      "260500 done 4:55:43.953114\n",
      "261000 done 4:56:24.670464\n",
      "261500 done 4:57:05.622796\n",
      "262000 done 4:57:45.973986\n",
      "262500 done 4:58:26.885978\n",
      "263000 done 4:59:10.012433\n",
      "263500 done 4:59:48.133106\n",
      "264000 done 5:00:22.762652\n",
      "264500 done 5:01:04.032699\n",
      "265000 done 5:01:46.521469\n",
      "265500 done 5:02:33.701834\n",
      "266000 done 5:03:20.680063\n",
      "266500 done 5:04:09.314006\n",
      "267000 done 5:05:02.349530\n",
      "267500 done 5:06:00.646823\n",
      "268000 done 5:06:48.703603\n",
      "268500 done 5:07:33.493027\n",
      "269000 done 5:08:20.578786\n",
      "269500 done 5:09:06.140677\n",
      "270000 done 5:09:49.721685\n",
      "270500 done 5:10:35.770454\n",
      "271000 done 5:11:19.801572\n",
      "271500 done 5:12:00.279059\n",
      "272000 done 5:12:41.635012\n",
      "272500 done 5:13:18.891735\n",
      "273000 done 5:13:57.726991\n",
      "273500 done 5:14:45.373375\n",
      "274000 done 5:15:31.970208\n",
      "274500 done 5:16:08.458630\n",
      "275000 done 5:16:47.349495\n",
      "275500 done 5:17:28.053589\n",
      "276000 done 5:18:07.659951\n",
      "276500 done 5:18:50.852277\n",
      "277000 done 5:19:38.160926\n",
      "277500 done 5:20:20.042334\n",
      "278000 done 5:20:58.692349\n",
      "278500 done 5:21:38.869902\n",
      "279000 done 5:22:22.052181\n",
      "279500 done 5:23:05.888784\n",
      "280000 done 5:23:53.826791\n",
      "280500 done 5:24:35.286057\n",
      "281000 done 5:25:16.813798\n",
      "281500 done 5:25:59.694157\n",
      "282000 done 5:26:40.491645\n",
      "282500 done 5:27:28.438448\n",
      "283000 done 5:28:13.873143\n",
      "283500 done 5:28:52.809146\n",
      "284000 done 5:29:31.919361\n",
      "284500 done 5:30:13.060013\n",
      "285000 done 5:30:53.666278\n",
      "285500 done 5:31:38.710888\n",
      "286000 done 5:32:27.448479\n",
      "286500 done 5:33:07.410892\n",
      "287000 done 5:33:47.315509\n",
      "287500 done 5:34:25.262076\n",
      "288000 done 5:35:01.374743\n",
      "288500 done 5:35:42.080407\n",
      "289000 done 5:36:21.924882\n",
      "289500 done 5:37:11.834898\n",
      "290000 done 5:37:54.677163\n",
      "290500 done 5:38:35.299782\n",
      "291000 done 5:39:17.754690\n",
      "291500 done 5:39:57.145648\n",
      "292000 done 5:40:39.763441\n",
      "292500 done 5:41:23.298828\n",
      "293000 done 5:42:17.250790\n",
      "293500 done 5:43:04.120057\n",
      "294000 done 5:44:00.702687\n",
      "294500 done 5:44:51.121256\n",
      "295000 done 5:45:45.159631\n",
      "295500 done 5:46:33.674987\n",
      "296000 done 5:47:28.324983\n",
      "296500 done 5:48:22.714815\n",
      "297000 done 5:49:11.250405\n",
      "297500 done 5:50:03.189704\n",
      "298000 done 5:50:54.249350\n",
      "298500 done 5:51:46.034984\n",
      "299000 done 5:52:40.953260\n",
      "299500 done 5:53:31.171285\n",
      "300000 done 5:54:18.526810\n",
      "300500 done 5:55:07.634452\n",
      "301000 done 5:56:05.017653\n",
      "301500 done 5:56:54.438167\n",
      "302000 done 5:57:42.713859\n",
      "302500 done 5:58:37.214403\n",
      "303000 done 5:59:31.757262\n",
      "303500 done 6:00:29.280630\n",
      "304000 done 6:01:13.265272\n",
      "304500 done 6:01:57.560807\n",
      "305000 done 6:02:41.581318\n",
      "305500 done 6:03:27.666586\n",
      "306000 done 6:04:11.089080\n",
      "306500 done 6:04:55.594477\n",
      "307000 done 6:05:41.496243\n",
      "307500 done 6:06:25.190999\n",
      "308000 done 6:07:03.811985\n",
      "308500 done 6:07:47.322300\n",
      "309000 done 6:08:34.071927\n",
      "309500 done 6:09:17.079859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310000 done 6:10:04.794856\n",
      "310500 done 6:10:49.346114\n",
      "311000 done 6:11:32.873427\n",
      "311500 done 6:12:18.838272\n",
      "312000 done 6:13:04.364712\n",
      "312500 done 6:13:48.624563\n",
      "313000 done 6:14:21.415955\n",
      "313500 done 6:14:41.727619\n",
      "314000 done 6:15:00.328953\n",
      "314500 done 6:15:19.006661\n",
      "315000 done 6:15:36.015184\n",
      "315500 done 6:15:54.555331\n",
      "316000 done 6:16:14.960893\n",
      "316500 done 6:16:34.853175\n",
      "317000 done 6:16:53.474682\n",
      "317500 done 6:17:11.083887\n",
      "318000 done 6:17:29.469607\n",
      "318500 done 6:17:51.233045\n",
      "319000 done 6:18:09.342449\n",
      "319500 done 6:18:29.692998\n",
      "320000 done 6:18:53.197682\n",
      "320500 done 6:19:11.908469\n",
      "321000 done 6:19:31.087894\n",
      "321500 done 6:19:50.760850\n",
      "322000 done 6:20:13.730407\n",
      "322500 done 6:20:35.050396\n",
      "323000 done 6:20:55.345384\n",
      "323500 done 6:21:14.056157\n",
      "324000 done 6:21:33.505056\n",
      "324500 done 6:21:51.608768\n",
      "325000 done 6:22:11.125191\n",
      "325500 done 6:22:32.981068\n",
      "326000 done 6:22:51.810141\n",
      "326500 done 6:23:08.967608\n",
      "327000 done 6:23:25.825994\n",
      "327500 done 6:23:42.793252\n",
      "328000 done 6:23:59.742581\n",
      "328500 done 6:24:16.778581\n",
      "329000 done 6:24:34.678334\n",
      "329500 done 6:24:51.675668\n",
      "330000 done 6:25:09.051336\n",
      "330500 done 6:25:26.430626\n",
      "331000 done 6:25:44.381859\n",
      "331500 done 6:26:03.097013\n",
      "332000 done 6:26:20.940609\n",
      "332500 done 6:26:39.035421\n",
      "333000 done 6:26:57.429376\n",
      "333500 done 6:27:15.183660\n",
      "334000 done 6:27:32.966585\n",
      "334500 done 6:27:50.454888\n",
      "335000 done 6:28:08.104834\n",
      "335500 done 6:28:25.556181\n",
      "336000 done 6:28:43.901468\n",
      "336500 done 6:29:01.960508\n",
      "337000 done 6:29:21.213373\n",
      "337500 done 6:29:41.343454\n",
      "338000 done 6:30:00.591120\n",
      "338500 done 6:30:20.278541\n",
      "339000 done 6:30:40.042711\n",
      "339500 done 6:30:59.783677\n",
      "340000 done 6:31:20.550175\n",
      "340500 done 6:31:42.192602\n",
      "341000 done 6:32:02.646575\n",
      "341500 done 6:32:23.202519\n",
      "342000 done 6:32:46.605500\n",
      "342500 done 6:33:08.699589\n",
      "343000 done 6:33:30.715505\n",
      "343500 done 6:33:50.732380\n",
      "344000 done 6:34:11.966337\n",
      "344500 done 6:34:31.201802\n",
      "345000 done 6:34:50.090905\n",
      "345500 done 6:35:08.282510\n",
      "346000 done 6:35:27.265320\n",
      "346500 done 6:35:46.003073\n",
      "347000 done 6:36:04.727568\n",
      "347500 done 6:36:23.982003\n",
      "348000 done 6:36:43.133129\n",
      "348500 done 6:37:02.210110\n",
      "349000 done 6:37:20.984073\n",
      "349500 done 6:37:39.456733\n",
      "350000 done 6:37:57.875768\n",
      "350500 done 6:38:16.727911\n",
      "351000 done 6:38:35.685909\n",
      "351500 done 6:38:54.219078\n"
     ]
    }
   ],
   "source": [
    "# Takes approx. 2 hr 40 mins to finish\n",
    "sample_bert_embs = create_batch_bert_embs(all_sents, save_path=dpath + 'sample_' + model_name + '_embs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_bert_vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9321ca2cc7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mrc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhso_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdset_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_embs.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_bert_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisogyny_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_bert_vecs' is not defined"
     ]
    }
   ],
   "source": [
    "rc = 0\n",
    "with open(dpath + dset_names[0] + '/' + model_name + '_embs_train.pkl', 'wb') as outfile:\n",
    "    pickle.dump(sample_bert_embs[rc:rc+len(js_data[0]), :], outfile)\n",
    "rc += len(js_data[0])\n",
    "with open(dpath + dset_names[0] + '/' + model_name + '_embs_test.pkl', 'wb') as outfile:\n",
    "    pickle.dump(sample_bert_embs[rc:rc+len(js_data[1]), :], outfile)\n",
    "rc += len(js_data[1])\n",
    "with open(dpath + dset_names[1] + '/' + model_name + '_embs.pkl', 'wb') as outfile:\n",
    "    pickle.dump(sample_bert_embs[rc:rc+len(hs_data), :], outfile)\n",
    "rc += len(hs_data)\n",
    "with open(dpath + dset_names[2] + '/' + model_name + '_embs.pkl', 'wb') as outfile:\n",
    "    pickle.dump(sample_bert_embs[rc:rc+len(hso_data), :], outfile)\n",
    "rc += len(hso_data)\n",
    "with open(dpath + dset_names[0] + '/' + model_name + '_embs.pkl', 'wb') as outfile:\n",
    "    pickle.dump(sample_bert_vecs[rc:rc+len(misogyny_data)], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereoset Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_number = 1\n",
    "data_tuples = json.load(open(dpath + 'stereoset/simulated_data/blank_split_' + str(batch_number) + '.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_sents = []\n",
    "for i, data_eg in enumerate(data_tuples):\n",
    "    inp = data_eg['input']\n",
    "    comp = data_eg['suggestion']\n",
    "    data_sents.append(inp.strip() + ' ' + comp.strip())\n",
    "print(len(data_sents))\n",
    "data_bert_embs = create_batch_bert_embs(data_sents)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dpath + 'stereoset/simulated_data/split_' + str(batch_number) + '_' + model_name + '_data_embs.pkl', 'wb') as outfile:\n",
    "    pickle.dump(data_bert_embs, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Batched Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dpath + 'sample_' + model_name + '_embs.pkl', 'rb') as infile:\n",
    "    sample_bert_embs = pickle.load(infile)\n",
    "print(sample_bert_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dpath + 'stereoset/simulated_data/split_' + str(batch_number) + '_' + model_name + '_data_embs.pkl', 'rb') as infile:\n",
    "    data_bert_embs = pickle.load(infile)\n",
    "print(data_bert_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4056)\n",
    "train_x = []\n",
    "train_y = []\n",
    "dev_x = []\n",
    "dev_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i, data_eg in enumerate(data_tuples):\n",
    "    inp = data_eg['input']\n",
    "    comp = data_eg['suggestion']\n",
    "    label = data_eg['label']\n",
    "    if label == 'stereotype':\n",
    "        dy = torch.from_numpy(np.ones((1, 1), dtype=int))\n",
    "    else:\n",
    "        dy = torch.from_numpy(np.zeros((1, 1), dtype=int))\n",
    "\n",
    "    dx = data_bert_embs[i, :].view(1, -1)\n",
    "    toss = random.random()\n",
    "    \n",
    "    if toss <= 0.7:\n",
    "        train_x.append(dx)\n",
    "        train_y.append(dy)\n",
    "    elif toss <= 0.85:\n",
    "        dev_x.append(dx)\n",
    "        dev_y.append(dy)\n",
    "    else:\n",
    "        test_x.append(dx)\n",
    "        test_y.append(dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_data(data_x, data_y, task_type='classification', batch_size=64):\n",
    "    b = 0\n",
    "    e = batch_size\n",
    "    data_batches = []\n",
    "    y_dim = data_y[0].size(-1)\n",
    "    while b < len(data_x):\n",
    "        data_x[b:e]\n",
    "        d_X = torch.cat(data_x[b:e], dim=0).float()\n",
    "        if task_type == 'classification':\n",
    "            d_Y = torch.cat(data_y[b:e], dim=0).view(-1).long()\n",
    "        elif task_type == 'multilabel':\n",
    "            d_Y = torch.cat(data_y[b:e], dim=0).view(-1, y_dim).float()\n",
    "        data_batches.append((d_X, d_Y))\n",
    "        b += batch_size\n",
    "        e += batch_size\n",
    "    return data_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_tr_batches = batchify_data(train_x, train_y)\n",
    "ss_de_batches = batchify_data(dev_x, dev_y)\n",
    "ss_te_batches = batchify_data(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating multi-label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "random.seed(4056)\n",
    "dsets = {}\n",
    "\n",
    "#jigsaw data\n",
    "dset_name = dset_names[0]\n",
    "dsets[dset_name] = {}\n",
    "dsets[dset_name]['train'] = ([], [])\n",
    "dsets[dset_name]['dev'] = ([], [])\n",
    "dsets[dset_name]['test'] = ([], [])\n",
    "dset_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "for eg in js_data[0]:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.zeros((1, len(dset_labels)), dtype=int))\n",
    "    for lnum, label in enumerate(dset_labels):\n",
    "        if int(eg[label]) > 0:\n",
    "            dy[0, lnum] = 1\n",
    "    toss = random.random()\n",
    "    if toss <= 0.8:\n",
    "        dsets[dset_name]['train'][0].append(dx)\n",
    "        dsets[dset_name]['train'][1].append(dy)\n",
    "    else:\n",
    "        dsets[dset_name]['dev'][0].append(dx)\n",
    "        dsets[dset_name]['dev'][1].append(dy)\n",
    "    i += 1\n",
    "\n",
    "for eg in js_data[1]:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.zeros((1, len(dset_labels)), dtype=int))\n",
    "    for lnum, label in enumerate(dset_labels):\n",
    "        if int(eg[label]) > 0:\n",
    "            dy[0, lnum] = 1        \n",
    "    dsets[dset_name]['test'][0].append(dx)\n",
    "    dsets[dset_name]['test'][1].append(dy)\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "#hate-speech data\n",
    "dset_name = dset_names[1]\n",
    "dsets[dset_name] = {}\n",
    "dsets[dset_name]['train'] = ([], [])\n",
    "dsets[dset_name]['dev'] = ([], [])\n",
    "dsets[dset_name]['test'] = ([], [])\n",
    "dset_labels = ['hate']\n",
    "for eg in hs_data:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.zeros((1, len(dset_labels)), dtype=int))\n",
    "    for lnum, label in enumerate(dset_labels):\n",
    "        if int(eg[label]) > 0:\n",
    "            dy[0, lnum] = 1\n",
    "    toss = random.random()\n",
    "    if toss <= 0.7:\n",
    "        dsets[dset_name]['train'][0].append(dx)\n",
    "        dsets[dset_name]['train'][1].append(dy)\n",
    "    elif toss <= 0.85:\n",
    "        dsets[dset_name]['dev'][0].append(dx)\n",
    "        dsets[dset_name]['dev'][1].append(dy)\n",
    "    else:\n",
    "        dsets[dset_name]['test'][0].append(dx)\n",
    "        dsets[dset_name]['test'][1].append(dy)\n",
    "    i += 1\n",
    "    \n",
    "#hate-speech-and-offensive data\n",
    "dset_name = dset_names[2]\n",
    "dsets[dset_name] = {}\n",
    "dsets[dset_name]['train'] = ([], [])\n",
    "dsets[dset_name]['dev'] = ([], [])\n",
    "dsets[dset_name]['test'] = ([], [])\n",
    "dset_labels = ['class']\n",
    "for eg in hso_data:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.zeros((1, len(dset_labels)), dtype=int))\n",
    "    for lnum, label in enumerate(dset_labels):\n",
    "        if int(eg[label]) > 0:\n",
    "            dy[0, lnum] = int(eg[label])\n",
    "    toss = random.random()\n",
    "    if toss <= 0.7:\n",
    "        dsets[dset_name]['train'][0].append(dx)\n",
    "        dsets[dset_name]['train'][1].append(dy)\n",
    "    elif toss <= 0.85:\n",
    "        dsets[dset_name]['dev'][0].append(dx)\n",
    "        dsets[dset_name]['dev'][1].append(dy)\n",
    "    else:\n",
    "        dsets[dset_name]['test'][0].append(dx)\n",
    "        dsets[dset_name]['test'][1].append(dy) \n",
    "    i += 1\n",
    "\n",
    "#misogyny data\n",
    "dset_name = dset_names[3]\n",
    "dsets[dset_name] = {}\n",
    "dsets[dset_name]['train'] = ([], [])\n",
    "dsets[dset_name]['dev'] = ([], [])\n",
    "dsets[dset_name]['test'] = ([], [])\n",
    "dset_labels = ['misogynous']\n",
    "for eg in misogyny_data:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.zeros((1, len(dset_labels)), dtype=int))\n",
    "    for lnum, label in enumerate(dset_labels):\n",
    "        if int(eg[label]) > 0:\n",
    "            dy[0, lnum] = 1\n",
    "    toss = random.random()\n",
    "    if toss <= 0.7:\n",
    "        dsets[dset_name]['train'][0].append(dx)\n",
    "        dsets[dset_name]['train'][1].append(dy)\n",
    "    elif toss <= 0.85:\n",
    "        dsets[dset_name]['dev'][0].append(dx)\n",
    "        dsets[dset_name]['dev'][1].append(dy)\n",
    "    else:\n",
    "        dsets[dset_name]['test'][0].append(dx)\n",
    "        dsets[dset_name]['test'][1].append(dy)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dsets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dsets = {}\n",
    "for label in dsets:\n",
    "    if label == 'jigsaw-dataset':\n",
    "        tr_batches = batchify_data(dsets[label]['train'][0], dsets[label]['train'][1], task_type='multilabel')\n",
    "        de_batches = batchify_data(dsets[label]['dev'][0], dsets[label]['dev'][1], task_type='multilabel')\n",
    "        te_batches = batchify_data(dsets[label]['test'][0], dsets[label]['test'][1], task_type='multilabel')\n",
    "    else:\n",
    "        tr_batches = batchify_data(dsets[label]['train'][0], dsets[label]['train'][1], task_type='classification')\n",
    "        de_batches = batchify_data(dsets[label]['dev'][0], dsets[label]['dev'][1], task_type='classification')\n",
    "        te_batches = batchify_data(dsets[label]['test'][0], dsets[label]['test'][1], task_type='classification')\n",
    "    batched_dsets[label] = (tr_batches, de_batches, te_batches)\n",
    "batched_dsets['stereotype'] = (ss_tr_batches, ss_de_batches, ss_te_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dpath + 'batched_dsets_multilabel_' + model_name + 'bsz64.pkl', 'wb') as outfile:\n",
    "    pickle.dump(batched_dsets, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batched_dsets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating binary-label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "random.seed(4056)\n",
    "dsets = {}\n",
    "\n",
    "for eg in js_data[0]:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    for label in eg:\n",
    "        if label not in ['id', 'comment_text']:\n",
    "            if label not in dsets:\n",
    "                dsets[label] = {}\n",
    "                dsets[label]['train'] = ([], [])\n",
    "                dsets[label]['dev'] = ([], [])\n",
    "                dsets[label]['test'] = ([], [])\n",
    "            if eg[label] == 1:\n",
    "                dy = torch.from_numpy(np.ones((1, 1), dtype=int))\n",
    "            else:\n",
    "                dy = torch.from_numpy(np.zeros((1, 1), dtype=int))\n",
    "            toss = random.random()\n",
    "            if toss <= 0.7:\n",
    "                dsets[label]['train'][0].append(dx)\n",
    "                dsets[label]['train'][1].append(dy)\n",
    "            elif toss <= 0.85:\n",
    "                dsets[label]['dev'][0].append(dx)\n",
    "                dsets[label]['dev'][1].append(dy)\n",
    "            else:\n",
    "                dsets[label]['test'][0].append(dx)\n",
    "                dsets[label]['test'][1].append(dy)\n",
    "    i += 1\n",
    "\n",
    "for eg in js_data[1]:\n",
    "    i += 1\n",
    "    \n",
    "for eg in hs_data:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    for label in eg:\n",
    "        if label not in ['id', 'comment_text', 'num_contexts']:\n",
    "            if label not in dsets:\n",
    "                dsets[label] = {}\n",
    "                dsets[label]['train'] = ([], [])\n",
    "                dsets[label]['dev'] = ([], [])\n",
    "                dsets[label]['test'] = ([], [])\n",
    "            if eg[label] == 1:\n",
    "                dy = torch.from_numpy(np.ones((1, 1), dtype=int))\n",
    "            else:\n",
    "                dy = torch.from_numpy(np.zeros((1, 1), dtype=int))\n",
    "            toss = random.random()\n",
    "            if toss <= 0.7:\n",
    "                dsets[label]['train'][0].append(dx)\n",
    "                dsets[label]['train'][1].append(dy)\n",
    "            elif toss <= 0.85:\n",
    "                dsets[label]['dev'][0].append(dx)\n",
    "                dsets[label]['dev'][1].append(dy)\n",
    "            else:\n",
    "                dsets[label]['test'][0].append(dx)\n",
    "                dsets[label]['test'][1].append(dy)\n",
    "    i += 1\n",
    "    \n",
    "label = 'hate_speech_offensive'\n",
    "dsets[label] = {}\n",
    "dsets[label]['train'] = ([], [])\n",
    "dsets[label]['dev'] = ([], [])\n",
    "dsets[label]['test'] = ([], [])\n",
    "for eg in hso_data:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.ones((1, 1), dtype=int))\n",
    "    dy[0, 0] = int(eg['class'])\n",
    "    toss = random.random()\n",
    "    if toss <= 0.7:\n",
    "        dsets[label]['train'][0].append(dx)\n",
    "        dsets[label]['train'][1].append(dy)\n",
    "    elif toss <= 0.85:\n",
    "        dsets[label]['dev'][0].append(dx)\n",
    "        dsets[label]['dev'][1].append(dy)\n",
    "    else:\n",
    "        dsets[label]['test'][0].append(dx)\n",
    "        dsets[label]['test'][1].append(dy)\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "label = 'misogyny'\n",
    "dsets[label] = {}\n",
    "dsets[label]['train'] = ([], [])\n",
    "dsets[label]['dev'] = ([], [])\n",
    "dsets[label]['test'] = ([], [])\n",
    "for eg in misogyny_data:\n",
    "    dx = sample_bert_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.ones((1, 1), dtype=int))\n",
    "    dy[0, 0] = int(eg['misogynous'])\n",
    "    toss = random.random()\n",
    "    if toss <= 0.7:\n",
    "        dsets[label]['train'][0].append(dx)\n",
    "        dsets[label]['train'][1].append(dy)\n",
    "    elif toss <= 0.85:\n",
    "        dsets[label]['dev'][0].append(dx)\n",
    "        dsets[label]['dev'][1].append(dy)\n",
    "    else:\n",
    "        dsets[label]['test'][0].append(dx)\n",
    "        dsets[label]['test'][1].append(dy)\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dsets = {}\n",
    "for label in dsets:\n",
    "    tr_batches = batchify_data(dsets[label]['train'][0], dsets[label]['train'][1])\n",
    "    de_batches = batchify_data(dsets[label]['dev'][0], dsets[label]['dev'][1])\n",
    "    te_batches = batchify_data(dsets[label]['test'][0], dsets[label]['test'][1])\n",
    "    batched_dsets[label] = (tr_batches, de_batches, te_batches)\n",
    "batched_dsets['stereotype'] = (ss_tr_batches, ss_de_batches, ss_te_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dpath + 'batched_dsets.pkl', 'wb') as outfile:\n",
    "    pickle.dump(batched_dsets, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(batched_dsets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTurk Annotated Data Processing and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = list(csv.reader(open(dpath + 'mturk_annotation/mturk_batch1_output.csv', 'r')))\n",
    "f2 = list(csv.reader(open(dpath + 'mturk_annotation/mturk_batch2_output.csv', 'r')))\n",
    "f3 = list(csv.reader(open(dpath + 'mturk_annotation/mturk_batch3_output.csv', 'r')))\n",
    "f4 = list(csv.reader(open(dpath + 'mturk_annotation/filtered_batch1_output.csv', 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1[0])\n",
    "print(f2[0])\n",
    "print(f3[0])\n",
    "print(f4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for row in f1[1:]:\n",
    "    data[row[0]] = (row[1], 'no')\n",
    "for row in f2[1:]:\n",
    "    data[row[1]] = (row[2], row[5])\n",
    "for row in f3[1:]:\n",
    "    data[row[0]] = (row[1], row[2])\n",
    "for row in f4[1:]:\n",
    "    data[row[0]] = (row[1], row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_data = {}\n",
    "for sent in data:\n",
    "    if data[sent][0] not in ['yes', 'no'] or data[sent][1] not in ['yes', 'no']:\n",
    "        pass\n",
    "#         print(sent)\n",
    "##  Uncomment to self-annotate examples that don't have agreement\n",
    "#         a1 = data[sent][0]\n",
    "#         a2 = data[sent][1]\n",
    "#         if data[sent][0] not in ['yes', 'no']:\n",
    "#             a1 = input('Explicit?: ')\n",
    "#         if data[sent][1] not in ['yes', 'no']:\n",
    "#             a2 = input('Implicit?: ')\n",
    "#         filt_data[sent] = (a1, a2)\n",
    "    else:\n",
    "        filt_data[sent] = data[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filt_data))\n",
    "s = 0\n",
    "u = 0\n",
    "n = 0\n",
    "for sent in filt_data:\n",
    "    if filt_data[sent][0] == 'yes':\n",
    "        s += 1\n",
    "    if filt_data[sent][1] == 'yes':\n",
    "        u += 1\n",
    "    if filt_data[sent][0] == 'no' and filt_data[sent][1] == 'no':\n",
    "        n += 1\n",
    "print(s, u, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the model loading and function definition files from 'BERT Embedding Computation' section before running this cell\n",
    "all_sents = []\n",
    "for sent in filt_data:\n",
    "    all_sents.append(sent)\n",
    "all_embs = create_batch_bert_embs(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stereotype-gold data binary classification task data tensor generation\n",
    "mturk_dsets = {}\n",
    "dset_name = 'stereotype-gold-binary'\n",
    "mturk_dsets[dset_name] = {}\n",
    "mturk_dsets[dset_name]['train'] = ([], [])\n",
    "mturk_dsets[dset_name]['dev'] = ([], [])\n",
    "mturk_dsets[dset_name]['test'] = ([], [])\n",
    "\n",
    "i = 0\n",
    "for eg in all_sents:\n",
    "    dx = all_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.zeros((1, 1), dtype=int))\n",
    "                          \n",
    "    if filt_data[eg][0] == 'yes' or filt_data[eg][1] == 'yes':\n",
    "        dy[0, 0] = 1\n",
    "                        \n",
    "    toss = random.random()\n",
    "    if toss <= 0.7:\n",
    "        mturk_dsets[dset_name]['train'][0].append(dx)\n",
    "        mturk_dsets[dset_name]['train'][1].append(dy)\n",
    "    elif toss <= 0.85:\n",
    "        mturk_dsets[dset_name]['dev'][0].append(dx)\n",
    "        mturk_dsets[dset_name]['dev'][1].append(dy)\n",
    "    else:\n",
    "        mturk_dsets[dset_name]['test'][0].append(dx)\n",
    "        mturk_dsets[dset_name]['test'][1].append(dy)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stereotype-gold data multilabel task data tensor generation\n",
    "dset_name = 'stereotype-gold-multilabel'\n",
    "mturk_dsets[dset_name] = {}\n",
    "mturk_dsets[dset_name]['train'] = ([], [])\n",
    "mturk_dsets[dset_name]['dev'] = ([], [])\n",
    "mturk_dsets[dset_name]['test'] = ([], [])\n",
    "\n",
    "i = 0\n",
    "for eg in all_sents:\n",
    "    dx = all_embs[i, :].view(1, -1)\n",
    "    dy = torch.from_numpy(np.zeros((1, 2), dtype=int))\n",
    "                          \n",
    "    if filt_data[eg][0] == 'yes':\n",
    "        dy[0, 0] = 1\n",
    "    if filt_data[eg][1] == 'yes':\n",
    "        dy[0, 1] = 1\n",
    "                        \n",
    "    toss = random.random()\n",
    "    if toss <= 0.7:\n",
    "        mturk_dsets[dset_name]['train'][0].append(dx)\n",
    "        mturk_dsets[dset_name]['train'][1].append(dy)\n",
    "    elif toss <= 0.85:\n",
    "        mturk_dsets[dset_name]['dev'][0].append(dx)\n",
    "        mturk_dsets[dset_name]['dev'][1].append(dy)\n",
    "    else:\n",
    "        mturk_dsets[dset_name]['test'][0].append(dx)\n",
    "        mturk_dsets[dset_name]['test'][1].append(dy)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run batchify_data() from 'Create Batched Datasets' section before running this cell\n",
    "mturk_batched_dsets = {}\n",
    "for label in mturk_dsets:\n",
    "    if label.endswith('multilabel'):\n",
    "        tr_batches = batchify_data(mturk_dsets[label]['train'][0], mturk_dsets[label]['train'][1], task_type='multilabel')\n",
    "        de_batches = batchify_data(mturk_dsets[label]['dev'][0], mturk_dsets[label]['dev'][1], task_type='multilabel')\n",
    "        te_batches = batchify_data(mturk_dsets[label]['test'][0], mturk_dsets[label]['test'][1], task_type='multilabel')\n",
    "    else:\n",
    "        tr_batches = batchify_data(mturk_dsets[label]['train'][0], mturk_dsets[label]['train'][1], task_type='classification')\n",
    "        de_batches = batchify_data(mturk_dsets[label]['dev'][0], mturk_dsets[label]['dev'][1], task_type='classification')\n",
    "        te_batches = batchify_data(mturk_dsets[label]['test'][0], mturk_dsets[label]['test'][1], task_type='classification')\n",
    "    mturk_batched_dsets[label] = (tr_batches, de_batches, te_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dpath + 'mturk_batched_dsets_multilabel_' + model_name + '_bsz64.pkl', 'wb') as outfile:\n",
    "    pickle.dump(mturk_batched_dsets, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
